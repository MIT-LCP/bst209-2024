---
title: 'Regularization'
output:
  html_document: default
  pdf_document: default
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(caret)
library(glmnet)
library(GGally)
library(dslabs)
library(tableone)
library(knitr)
ds_theme_set()
```

The prediction error for any machine learning model can be decomposed into 3 components: **bias**, **variance**, and **irreducible error**. 

> Total error = Variance + Bias squared + Irreducible Error

In mathematical terms:

$$
\mbox{E}[y_0 - \hat f(x_0)]^2 = Var(\hat f(x_0)) + [Bias(\hat f(x_0))]^2 + Var(\epsilon).
$$

Irreducible error is the measure of noise in our data that cannot be reduced by improving the model. However, we can create a model with low bias and low variance.

These quantities are not independent; decreasing one will often increase the other, leading to the bias-variance tradeoff. We need to find a balance point to minimize both bias and variance.

# Bias-Variance Tradeoff 

* `Bias`: The error introduced by approximating a real-life problem, which may be complex, by a simpler model. For example, linear regression assumes a linear relationship between $Y$ and $X_1, X_2,...,X_p$, which may not hold true, resulting in bias. In other words, building a model that is too simple for the task at hand will result in high bias, whereas a more flexible (complex) model will decrease bias. 

* `Variance`: The amount by which $\hat f$ would change if we estimated it using a different training dataset. High variance indicates that small changes in the training data can lead to large changes in $\hat f$. In general, more flexible statistical methods have higher variance.

![](./images/bias-var-graphs.png)

As a general rule, more flexible methods increase variance but decrease bias. Initially, increasing flexibility reduces bias faster than it increases variance, leading to lower expected test set error. However, beyond a certain point, further increasing flexibility increases variance significantly.

Good test set performance requires low variance and low squared bias. The challenge is finding a method that minimizes both. 

Often it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high bias (by fitting a horizontal line to the data that does not follow a linear pattern). This is the trade-off.

# Regularization

In regression, the standard linear model $Y = \beta_0 + \beta_1X_1 + ... + \beta_pX_p$ is typically fit using the methods of ordinary least squares (OLS). Many datasets, however, have many predictors, some of which may not be predictive, increasing variance. 

Regularization techniques help to improve model performance and interpretability, while decreasing variance. We'll look at a popular approach for regularization known as "shrinkage".
Shrinkage methods reduce variance by shrinking coefficient estimates toward zero. Examples include ridge regression and lasso.

### Ridge regression

If we have $n$ observations and want to predict a response variable $Y$ using linear regression on $p$ covariates $X_1, X_2,...,X_p$, we are trying to find the coefficients $\beta_0, \beta_1, \beta_2, ..., \beta_p$ ($\beta_0$ is the intercept) that minimize the residual sum of squares (RSS): 

$$
\text{RSS} = \sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2
$$

Ridge regression modifies this by adding a penalty term to the RSS to shrink the coefficients:

$$
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p \beta_j^2
$$

The penalty term $\lambda \sum_{j=1}^p \beta_j^2$ is known as the L2 norm. This term shrinks the coefficients towards zero, which helps to reduce variance. The tuning parameter $\lambda$ controls the trade-off between fitting the data well (low bias) and keeping the coefficients small (low variance).

- When $\lambda = 0$, ridge regression is equivalent to OLS, leading to unbiased but high variance estimates.
- As $\lambda$ increases, the coefficients shrink more, increasing bias but reducing variance.

### Lasso

Lasso regression is similar to ridge regression but uses a different penalty term:

$$
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p |\beta_j|
$$
So instead of summing over the squares of the $\beta_j$s (the $L_2$ norm) in the shrinkage penalty term, lasso sums over their absolute values (the $L_1$ norm).

The penalty term $\lambda \sum_{j=1}^p |\beta_j|$ is known as the L1 norm. Unlike ridge regression, which shrinks coefficients continuously, lasso can shrink some coefficients to *exactly* zero, effectively performing __variable selection__.

- Lasso encourages sparsity, making the model easier to interpret by selecting a smaller subset of predictors.
- The choice of $\lambda$ balances the trade-off between variance reduction and bias introduction.
-  As $\lambda$ increases, more and more of the coefficients will be exactly zero, so unlike ridge regression, lasso performs variable selection!

Because lasso models for a given problem tend to have fewer predictors, they are sometimes called *sparse models* and are often easier to interpret than ridge regression models.


## $K$-fold cross-validation

In both ridge regression and LASSO, cross-validation is used to find the best value of $\lambda$. $K$-fold cross validation proceeds as follows: 

1. Choose a grid of $\lambda$ values that you would like to try. 
2. Randomly split your dataset into $K$ *folds*. Popular choices for $K$ are 10, 5 or even the number of observations $n$. 
3. For each fold:
    a. Use the fold as the validation set.
    b. Use the remaining folds as the training set.
    c. Fit the model for each $\lambda$ value.
    d. Evaluate the model and calculate the error. 
4. Compute the mean error for each $\lambda$.
5. Select the $\lambda$ with the smallest mean error.
6. Refit the model using the entire dataset with the chosen $\lambda$.

## Example: MEPS hospitalization data

The Medical Expenditure Panel Survey (MEPS), conducted by the Agency for Healthcare Research and Quality, contains longitudinal information on study participants.

Variables include age, sex, race, marital status, family income, education, type of insurance coverage, prior diagnosis with angina, arthritis, asthma, cancer, coronary heart disease, diabetes, emphysema, heart attack, high cholesterol, high blood pressure, heart disease, other heart disease, or stroke, perceived general and mental health status, and any hospitalization in 2016. 

We aim to predict 2017 hospitalization rates based on 2016 data. The data loaded below is a subset of 1000 individuals from the MEPS 2016â€“2017 panel data who were aged 18 years or older in 2016. This example is loosely adapted from *Statistics for Health Data Science*, by Ruth Etzioni, Micha Mandel, and Roman Gulati (2020). For more information about MEPS and to pull the entire original database, please go [here](https://roman-gulati.github.io/statistics-for-health-data-science/). 

```{r}
# Read in subset of MEPS data
load("meps.rData")
# Predictor variables
predictors = setdiff(names(meps), "Anyhosp2017")
```

## Exploring the Data

Let's take a quick look at the data using the `CreateTableOne` function we used earlier this week.

```{r}
# Create a table summarizing the dataset
table1 <- CreateTableOne(vars = predictors, data = meps, strata = "Sex")
kableone(table1)
```

We'll now prepare a "design matrix" (tabular structure containing our predictor variables) and split it into training and test sets.

Note that the algorithm we will be using, `glmnet`, expects you to pass in the predictors as a numeric model matrix. It can't handle categorical variables coded as factors. The `model.matrix` function creates the appropriate model matrix by converting factors into dummy variables as necessary. 

In the call below, `~ .-1` tells `model.matrix` to include all predictors except the intercept in the design matrix (by default, `glmnet` will include an intercept, so it's unneeded here). There are a total of 43 columns in the model matrix, compared to the 25 original predictors that include factor categorical variables. As usual, we split the data into 50% training and 50% test. 

```{r}
# Create design matrix for predictors (except for intercept)
X = model.matrix(~ .-1, meps %>% select(all_of(predictors)))
# Response variable
y = meps$Anyhosp2017

# Split the data into a training set and a test set
set.seed(219)
train_idx = createDataPartition(y, times = 1, p = 0.5, list = FALSE)
X_train = X[train_idx,]
y_train = y[train_idx]
X_test = X[-train_idx,]
y_test = y[-train_idx]
```

We're ready to try fitting some models using `glmnet`! Our response variable is binary, so we want to specify `family = "binomial"`, similarly to the `glm` function. (The default is `"gaussian"`, for regression.) If we pass in `lambda = 0`, this call is equivalent to the usual logistic regression. (Why?)

```{r}
fit_logistic = glmnet(X_train, y_train, family = "binomial", lambda = 0)
```

In order to actually shrink our coefficients, we need to pass in one or more non-zero values to the `lambda` argument. Or, if we don't specify anything at all, `glmnet` will automatically generate a grid of 100 $\lambda$ values to try. The key parameter to watch out for is `alpha`. If we want to fit a ridge regression model, we set `alpha = 1`. If we want lasso, we set `alpha = 0`. 

```{r}
fit_ridge = glmnet(X_train, y_train, family = "binomial", alpha = 1)
fit_lasso = glmnet(X_train, y_train, family = "binomial", alpha = 0)
```

`glmnet` has many, many more arguments that you can play around with, but the defaults are usually reasonable for most scenarios. Calling `plot` on a fitted `glmnet` object will plot the coefficients for the sequence of $\lambda$ values used by the model.

Setting `xvar = "lambda"` plots the log of the $\lambda$s on the x-axis; see the documentation for `plot.glmnet` for other x-axis options. The ridge regression coefficient plot shows most of the coefficients starting out at non-zero values for small values of lambda, but they eventually all get shrunk to zero when lambda is sufficiently large. 

```{r}
plot(fit_ridge, xvar = "lambda")
```

You can see a similar trend for the lasso coefficient plot, but the coefficients get shrunk to zero much more aggressively. 

```{r}
plot(fit_lasso, xvar = "lambda")
```

## Cross-validation

`cv.glmnet` is a built-in function that will run cross-validation to identify the best tuning parameter (lambda) for your model. The model specification and arguments are very similar to those for `glmnet`. By default, `cv.glmnet` uses 10-fold cross-validation.

```{r}
fit_ridge = cv.glmnet(X_train, y_train, family = "binomial", alpha = 1)
fit_lasso = cv.glmnet(X_train, y_train, family = "binomial", alpha = 0)
```


## Root mean-square error (RMSE)

To evaluate our three models' performances on the test set, we will consider the root mean square error (RMSE). If $\hat{y_i}$ is our predicted probability and $y_i$ is the true label (hospitalized in 2017 or not), then we would prefer the model that minimizes: 

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n \left( y_{i} - \hat{y}_{i} \right)^2}$$

We can interpret RMSE as the typical error that we make when predicting hospitalization. Various R packages include built-in functions to return the RMSE, but it is straightforward to write our own RMSE function and apply it to the test set predictions: 

```{r, eval=FALSE}
RMSE = function(true_labels, preds){
  sqrt(mean((true_labels - preds)^2))
}

sapply(list(Logistic = fit_logistic, Ridge = fit_ridge, Lasso = fit_lasso), 
       function(mod) {
  preds = predict(mod, newx = X_test, type = "response")
  return(RMSE(y_test, preds))
})
```

While it is true that the logistic regression test RMSE is the worst (largest) and the lasso test RMSE is the best (smallest), all three models appear to be performing similarly well.

Probably the greatest advantage of using the lasso model in this case is not so much due to performance gains as it is having a much smaller and more interpretable model. 

## Overall Accuracy

We can also calculate the confusion matrices and overall accuracy for each model.

```{r}
# Logistic model
y_hat_logit <- predict(fit_logistic, X_test, type="response")
y_hat_logit = round(y_hat_logit,0)

tab <- table(pred = y_hat_logit, truth = as.numeric(y_test))

confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

```{r}
# Ridge model
y_hat_ridge <- predict(fit_ridge, X_test, type="response", s=fit_ridge$lambda.min)
y_hat_ridge = round(y_hat_ridge,0)

tab <- table(pred = y_hat_ridge, truth = as.numeric(y_test))

confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

```{r}
# LASSO model
y_hat_lasso <- predict(fit_lasso, X_test, type="response", s=fit_lasso$lambda.min)
y_hat_lasso = round(y_hat_lasso,0)

tab <- table(pred = y_hat_lasso, truth = as.numeric(y_test))

confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```
We see that logisitic regression has the smallest accuracy and LASSO has the largest, but all 3 models are performing similarly. 

