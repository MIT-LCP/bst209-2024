---
title: 'Data preparation'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
library(caret)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(tableone)
library(knitr)
```

Critical care units are home to sophisticated monitoring systems, helping carers to support the lives of the sickest patients within a hospital. These monitoring systems produce large volumes of data that could be used to improve patient care.

![](./images/icu_patient.png)

Our goal will be to predict the outcome of critical care patients using physiological data available on the first day of admission to the intensive care unit. These predictions could be used for resource planning or to assist with family discussions.

The dataset used in this lesson was extracted from the [eICU Collaborative Research Database](https://eicu-crd.mit.edu/), a publicly available dataset comprising deidentified physiological data collected from critically ill patients.

## Sourcing and accessing data

Machine learning helps us to find patterns in data, so sourcing and understanding data is key. Unsuitable or poorly managed data will lead to a poor project outcome, regardless of the modelling approach.

We will be using an open access subset of the eICU Collaborative Research Database, a publicly available dataset comprising deidentified physiological data collected from critically ill patients. 

For simplicity, we will be working with a pre-prepared CSV file that comprises data extracted from a demo version of the dataset.

## Structured query language

Learning to extract data from sources such as databases and file systems is a key skill in machine learning. Familiarity with Structured Query Language (SQL) will equip you well for these tasks. For reference, the query used to extract the dataset is outlined below. Briefly, this query:

- `SELECTs` multiple columns
- `FROM` the `patient`, `apachepatientresult`, and `apacheapsvar` tables
- `WHERE` certain conditions are met.

```sql
SELECT p.gender, SAFE_CAST(p.age as int64) as age, p.admissionweight,
       a.unabridgedhosplos, a.acutephysiologyscore, a.apachescore, a.actualhospitalmortality,
       av.heartrate, av.meanbp, av.creatinine, av.temperature, av.respiratoryrate,
       av.wbc, p.admissionheight
FROM `physionet-data.eicu_crd_demo.patient` p
INNER JOIN `physionet-data.eicu_crd_demo.apachepatientresult` a
ON p.patientunitstayid = a.patientunitstayid
INNER JOIN `physionet-data.eicu_crd_demo.apacheapsvar` av
ON p.patientunitstayid = av.patientunitstayid
WHERE apacheversion LIKE 'IVa'
```

Letâ€™s begin by loading this data:

```{r}
# load the data
cohort <- read_csv("./eicu_cohort.csv")
head(cohort)
```

## Exercise

A) The output of `head(cohort)` isn't very friendly. Use the `kable` function to print a cleaner table.

B) It's common for ML papers to be written in LaTeX. How would you output the table in LaTeX? Hint: check the kable documentation with the help() function.

C) What's the average age of patients? How old are the youngest and eldest patients? How are missing values coded?

D) Plot a distribution of the heart rates. Do you notice anything unusual?

## Answers

A) Wrapping the existing head() code in kable should produce a cleaner output.

B) Use the help() function on kable will show you the documentation. Adding `format="latex"` should convert the output. Note: you may need to call print() on your function call to see the latex output.

C) Use `summary()` or `min()`, `max()`, and `median()`. Missing values appear to be coded as `-1`.

D) Try: `ggplot(cohort, aes(x = heartrate)) + 
          geom_density(fill = "blue", alpha = 0.5) +
          labs(title = "Heart rate distribution", x = "Heart rate", y = "Density")`
   Yes! The plot is bimodal.

## Knowing your data

Before moving ahead on a project, it is important to understand your data. Having someone with domain knowledge - and ideally first hand knowledge of the data collection process - helps us to design a sensible task and to use data effectively.

Summarizing data is an important first step. We will want to know aspects of the data such as: extent of missingness; data types; numbers of observations.

One common step is to view summary statistics. Often, the first table in a scientific paper is these summary statistics (for example, see [Table 1](https://www.nature.com/articles/s41746-018-0029-1/tables/1) of the paper by Rajkomar et al.). 

Let's generate a similar table for ourselves, using the [TableOne](https://cran.r-project.org/web/packages/tableone/vignettes/introduction.html) package:


```{r}
# rename columns
names(cohort)[names(cohort) == "unabridgedhosplos"] <- "length_of_stay"
names(cohort)[names(cohort) == "meanbp"] <- "mean_blood_pressure"
names(cohort)[names(cohort) == "wbc"] <- "white_cell_count"

#view summary characteristics
t1 <- CreateTableOne(vars = names(cohort), data = cohort, factorVars = "actualhospitalmortality")

#Output to LaTeX
kableone(t1)
```

## Exercise

A) What is the approximate percent mortality in the eICU cohort?  

B) Which variables appear noticeably different in the "Alive" (0) and "Deceased" (1) groups? Hint: Check the tableone documentation with `help()` to stratify the data.

C) How does the in-hospital mortality differ between the eICU cohort and the ones in [Rajkomar et al](https://www.nature.com/articles/s41746-018-0029-1/tables/1)?  

## Solution

A) Approximately 9% (48/536) 

B) After stratifying by `actualhospitalmortality`, you'll see that several variables differ, including age, length of stay, acute physiology score, heart rate, etc. 

C) The Rajkomar dataset has significantly lower in-hospital mortality (~2% vs 9%).

## Partitioning

Typically we will want to split our data into a training set and "held-out" test set. The training set is used for building our model and our test set is used for evaluation. A split of ~70% training, 30% test is common.

![Train and test set](./images/train_test.png)

To ensure reproducibility, we should set the random state of the splitting method. This means that the same "random" split  will be produced in the future.

Let's partition our data into training and test sets:

```{r}
# Set the seed
set.seed(331)

# Define the feature and outcome variable names
features <- "apachescore"
outcome <- "actualhospitalmortality"

# Create a partition index for training data
# (70% of the data) based on the outcome variable
train_index <- createDataPartition(cohort[[outcome]], p = 0.7, list = FALSE)

# Extract training data for features and outcome
x_train <- cohort[train_index, features]
y_train <- cohort[train_index, outcome]

# Extract testing data for features and outcome
x_test <- cohort[-train_index, features]
y_test <- cohort[-train_index, outcome]

# Create training and testing dataframes
train_df <- cohort[train_index, ]
test_df <- cohort[-train_index, ]
```

## Normalization

To ensure that the features are on a similar scale, we can normalize the data. Normalization can help improve the performance of machine learning algorithms.

A popular scaler is the Standard Scaler. This formula standardizes a feature $x$ by subtracting the mean $\mu$ and dividing by the standard deviation $\sigma$

$$z = \frac{x - \mu}{\sigma}$$
The scaler helps to ensure that the features in both the training and test sets have a mean, $\mu$, of 0 and a standard deviation, $\sigma$, of 1.

```{r}
# Define the preProcess function for standard scaling (zero mean, unit variance)
scaler <- preProcess(x_train, method = c("center", "scale"))

# Fit the scaler on the training dataset
x_train_scaled <- predict(scaler, x_train)

# Scale the test set using the same scaler
x_test_scaled <- predict(scaler, x_test)
```

## Exercise

A) What is the purpose of setting the seed? Why is this an important step to include?

B) How many features are included in our training data? How do we modify the code to include multiple features?

C) Why might we want to normalize data before feeding it into a machine learning model?

D) Verify that the normalization was applied correctly by checking the summary statistics of `x_train_scaled` and `x_test_scaled`.

E) Explain why it is important to fit the scaler on the training set only and then apply it to both the training and test sets. What issues might arise if you fit the scaler on the entire dataset before splitting into training and test sets?

## Solution

A) Setting the seed ensures that the random operations (such as data partitioning) produce the same result every time the code is run. This is crucial for reproducibility, allowing others to replicate your results and verify the findings.

B) Modify your `features` variable to take a list `c("apachescore", "age", "heartrate", ...)`.

C) Normalizing data ensures that all features are on a similar scale. This can help to improve the performance of machine learning algorithms. It helps to prevent features with larger scales from dominating the learning process.

D) Use of `summary()` and `std_dev()` should help you to confirm that the new data has been scaled appropriately.

E) It is important to fit the scaler on the training set only and then apply it to both the training and test sets to avoid data leakage. Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates and models that perform poorly in real-world scenarios. If you fit the scaler on the entire dataset before splitting into training and test sets, you incorporate information from the test set into the scaling parameters (e.g., mean and standard deviation).


