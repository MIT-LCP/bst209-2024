---
title: "Evaluation"
output: "html_document"
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1)
library(tidyverse)
library(caret)
library(dplyr)
library(dslabs)
library(rafalib)
ds_theme_set()
```

## Evaluating a classification task

We trained a machine learning model to predict the outcome of patients admitted to intensive care units. As there are two outcomes, we refer to this as a "binary" classification task. We are now ready to evaluate the model on our held-out test set. 

```{r}
set.seed(1)

# Load the data
cohort <- read_csv("./eicu_cohort.csv")
features = c("apachescore")

# Split the data into training and testing sets
train_index <- createDataPartition(cohort$apachescore, times = 1, p = 0.7, list = FALSE)
train_set <- cohort[train_index, ]
test_set <- cohort[-train_index, ]

# Fit the logistic regression model
log_model <- glm(actualhospitalmortality ~ apachescore, data = train_set, family = binomial)

# Generate predictions (and print the first 10 predictions)
y_hat_test <- predict(log_model, test_set[features], type="response")
y_hat_test[1:10]
```

Each prediction is assigned a probability of a positive class. For example, the first 10 probabilities are:

```{r}
probs = y_hat_test[1:10]
rounded_probs = round(probs,0)
print(rounded_probs)
```

These probabilities correspond to the following predictions, either a "0" ("ALIVE") or a 1 ("EXPIRED"):

In comparison with the known outcomes, we can put each prediction into one of the following categories:

- True positive: we predict "1" ("EXPIRED") and the true outcome is "1".
- True negative: we predict "0" ("ALIVE") and the true outcome is "0".
- False positive: we predict "1" ("EXPIRED") and the true outcome is "0".
- False negative: we predict "0" ("ALIVE") and the true outcome is "1".

## Confusion matrices

It is common practice to arrange these outcome categories into a "confusion matrix", which is a grid that records our predictions against the ground truth. For a binary outcome, confusion matrices are organised as follows:

|                        | Negative (predicted)   | Positive  (predicted) |
| :---                   | :----:                 | :----:                |
| Negative (actual)      | **TN**                 | FP                    |
| Positive (actual)      | FN                     | **TP**                |

The sum of the cells is the total number of predictions. The diagonal from top left to bottom right indicates correct predictions. Let's visualize the results of the model in the form of a confusion matrix:

```{r}
# Generate the confusion matrix
confusion <- confusionMatrix(as.factor(y_hat_test), as.factor(y_test))

# Print the confusion matrix
print(confusion$table)
```

![Confusion matrix](./images/section7-fig1.png){: width="600px"}

We have two columns and rows because we have a binary outcome, but you can also extend the matrix to plot multi-class classification predictions. If we had more output classes, the number of columns and rows would match the number of classes.

## Accuracy

Accuracy is the overall proportion of correct predictions. Think of a dartboard. How many shots did we take? How many did we hit? Divide one by the other and that's the accuracy. 

Accuracy can be written as:

$$
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
$$

What was the accuracy of our model? 

```{r}
acc = metrics.accuracy_score(y_test, y_hat_test)
print(f"Accuracy (model) = {acc:.2f}")
```

```
Accuracy (model) = 0.82
```

Not bad at first glance. When comparing our performance to guessing "0" for every patient, however, it seems slightly less impressive!

```{r}
zeros = np.zeros(len(y_test))
acc = metrics.accuracy_score(y_test, zeros)
print(f"Accuracy (zeros) = {acc:.2f}")
```

```
Accuracy (zeros) = 0.92
```

The problem with accuracy as a metric is that it is heavily influenced by prevalence of the positive outcome: because the proportion of 1s is relatively low, classifying everything as 0 is a safe bet.

We can see that the high accuracy is possible despite totally missing our target. To evaluate an algorithm in a way that prevalence does not cloud our assessment, we often look at sensitivity and specificity. 

## Sensitivity (A.K.A "Recall" and "True Positive Rate")

Sensitivity is the ability of an algorithm to predict a positive outcome when the actual outcome is positive. In our case, of the patients who die, what proportion did we correctly predict? This can be written as:

$$
Sensitivity = Recall = \frac{TP}{TP+FN}
$$

Because a model that calls "1" for everything has perfect sensitivity, this measure is not enough on its own. Alongside sensitivity we often report on specificity.

## Specificity (A.K.A "True Negative Rate")

Specificity relates to the test's ability to correctly classify patients who survive their stay (i.e. class "0"). Specificity is the proportion of those who survive who are predicted to survive. The formula for specificity is:

$$
Specificity = \frac{TN}{FP+TN}
$$

## Receiver-Operator Characteristic

A Receiver-Operator Characteristic (ROC) curve plots 1 - specificity vs. sensitivity at varying probability thresholds. The area under this curve is known as the AUROC (or sometimes just the "Area Under the Curve", AUC) and it is a well-used measure of discrimination that was originally developed by radar operators in the 1940s.

```{r}
metrics.plot_roc_curve(reg, x_test, y_test)
```
