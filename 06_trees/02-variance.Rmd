---
title: "Variance"
output: "html_document"
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(dplyr)
library(rafalib)
library(ggplot2)
library(gridExtra)
library(rpart)
library(rpart.plot)
```

```{r custom_plot, echo=FALSE, message=FALSE}
# Custom plotting function
# This function is reused in each workshop
plot_model_pred_2d <- function(model, dataset, feature1, feature2, target, title, min_f1, max_f1, min_f2, max_f2) {
  # Ensure dataset is a data frame
  if (!is.data.frame(dataset)) {
    dataset <- as.data.frame(dataset)
  }
  
  # Create a grid over the feature space
  grid <- expand.grid(
    feature1 = seq(min_f1, max_f1, length.out = 100),
    feature2 = seq(min_f2, max_f2, length.out = 100)
  )
  
  # Set correct column names for grid
  colnames(grid) <- c(feature1, feature2)
  
  # Predict class for each point in the grid using the model
  grid$predicted_class <- predict(model, newdata = grid, type = "class")
  
  plot <- ggplot() +
    geom_tile(data = grid, aes(x = .data[[feature2]], y = .data[[feature1]], fill = .data$predicted_class), alpha = 0.3) +
    geom_point(data = dataset, aes(x = .data[[feature2]], y = .data[[feature1]], color = .data[[target]]), size = 2) +
    labs(title = title, x = feature2, y = feature1) +
    scale_fill_manual(values = c("red", "blue"), name = "Predicted Class") +
    scale_color_manual(values = c("red", "blue"), name = "Actual Class") +
    theme_minimal()
  
    return(plot)
}
```

In the previous episode we created a very simple decision tree. Let's see what happens when we introduce new decision points by increasing the depth. First, let's load and split the data:

```{r load_data}
# Load the data
cohort <- read.csv("./eicu_cohort_trees.csv")
features = c("acutephysiologyscore", "age")
cohort$actualhospitalmortality = as.factor(cohort$actualhospitalmortality)

# Split the data into training and testing sets
set.seed(42)
train_index <- createDataPartition(cohort$acutephysiologyscore, times = 1, p = 0.7, list = FALSE)
train_set <- cohort[train_index, ]
test_set <- cohort[-train_index, ]
```

Now, let's increase the depth:

```{r}
# Create and fit the decision tree model
mdl <- rpart(actualhospitalmortality ~ age + acutephysiologyscore,
             data = train_set, method = "class",
             control = rpart.control(maxdepth = 5, minbucket = 1))
```

```{r}
txt = "Decision Boundary of the Decision Tree"
plot_model_pred_2d(mdl, train_set, "acutephysiologyscore", "age", "actualhospitalmortality",
                   title = txt, 0, 200, 0, 100)
```

Now our tree is more complicated! We can see a few vertical boundaries as well as the horizontal one from before (and it makes sense we see the original boundary, given we know our modelling decisions are **greedy**). Some of these we may like, but some appear unnatural. Let's look at the tree itself.

```{r}
# Visualize the decision tree using rpart.plot
rpart.plot(mdl, extra = 2)
```

## Exercise

1) Consider a patient aged 80 years with an acute physiology score of 100. Using the image of the tree, work through the nodes until your can make a prediction. What outcome does your model predict?

2) Does the decision that led to this final node seem sensible to you? Why?

## Solution

1) From the top of the tree, we would work our way down:
   acutePhysiologyScore < 55? No.
   acutePhysiologyScore < 111? Yes.
   age < 76? No
   acutePhysiologyScore < 79. Yes.
   acutePhysiologyScore >= 101? No.

2) Having an entire rule based upon this one observation seems silly, but it is perfectly logical at the moment. The only objective the algorithm cares about is minimizing the impurity.

## Pruning

Let's prune the model and look again.

```{r}
# Prune the rpart model
pruned_mdl <- prune(mdl, cp=0.05)
rpart.plot(pruned_mdl)
```
Above, we can see that our second tree is smaller in depth again. We can look at the decision surface for this tree:

```{r}
txt = "Pruned Tree"
plot_model_pred_2d(pruned_mdl, train_set, "acutephysiologyscore", "age", "actualhospitalmortality",
                   title = txt, 0, 200, 0, 100)
```

Our pruned decision tree has a more intuitive boundary, but does make some errors. We have reduced our performance in an effort to simplify the tree. This is the classic machine learning problem of trading off complexity with error.

Note that, in order to do this, we "invented" the complexity parameter. Why choose 0.05? The answer is: it depends on the dataset. Heuristically choosing these parameters can be time consuming, and we will see later on how gradient boosting elegantly handles this task.

## Decision trees have high “variance”

Decision trees have high “variance”. In this context, variance refers to a property of some models to have a wide range of performance given random samples of data. Let’s take a look at randomly slicing the data we have to see what that means.

```{r}
# List to store plots
set.seed(42)  # Set seed for reproducibility
for (i in 1:3) {
  # Generate indices in a random order
  idx <- sample(nrow(train_set))
  
  # Only use the first 200 indices
  idx <- idx[1:200]
  subset <- train_set[idx, ]
  
  # Initialize and train the model
  mdl <- rpart(actualhospitalmortality ~ acutephysiologyscore + age, data = subset, 
               method = "class",
               control = rpart.control(maxdepth = 5))
  
  # Plot model predictions in 2D
  txt <- paste('Random sample', i)
  plot <- plot_model_pred_2d(mdl, subset, "acutephysiologyscore", "age", "actualhospitalmortality", title = txt, 0, 200, 0, 100)
  print(plot)
}


```

Above we can see that we are using random subsets of data, and as a result, our decision boundary can change quite a bit. As you could guess, we actually don’t want a model that randomly works well and randomly works poorly.

There is an old joke: two farmers and a statistician go hunting. They see a deer: the first farmer shoots, and misses to the left. The next farmer shoots, and misses to the right. The statistician yells “We got it!”.

While it doesn’t quite hold in real life, it turns out that this principle does hold for decision trees. Combining them in the right way ends up building powerful models.

## Exercise

1) Why are decision trees considered have high variance?

2) An “ensemble” is the name used for a machine learning model that aggregates the decisions of multiple sub-models. Why might creating ensembles of decision trees be a good idea?

## Solution

1) Minor changes in the data used to train decision trees can lead to very different model performance.

2) By combining many of instances of “high variance” classifiers (decision trees), we can end up with a single classifier with low variance.

## Key points

- Overfitting is a problem that occurs when our algorithm is too closely aligned to our training data.

- Models that are overfitted may not generalise well to “unseen” data.

- Pruning is one approach for helping to prevent overfitting.

- By combining many of instances of “high variance” classifiers, we can end up with a single classifier with low variance.

