---
title: 'Lab: Machine Learning II'
output: html_document
---

# Decision Trees, Bagging, and Random Forests for Multi-Class Outcomes

In this lab, we will use gene expression data to classify tissue samples. The data can be loaded from the `dslabs` package by calling `data(tissue_gene_expression)`. `tissuesGeneExpression` is a list with two elements: 

- `x`: Numeric matrix with 189 rows and 500 columns. Each column contains gene expression measurements for a different gene. 
- `y`: Factor vector of length 189 that records tissue type labels (cerebellum, colon, endometrium, hippocampus, kidney, liver, or placenta) . 

The original data (accessible in the `tissuesGeneExpression` package) records gene expression for 22,215 genes. 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(GGally)
library(tree)
library(randomForest)

library(dslabs)
data(tissue_gene_expression)
```

We will only use a random sample of 100 genes to predict tissue type. 

```{r}
set.seed(25)
tiss_ge = data.frame(y = tissue_gene_expression$y, 
                     tissue_gene_expression$x[,sample(500, 100)])
```

As usual, we split the data into training and test sets, each with about 50% of the data. 

```{r}
set.seed(36)
tiss_ge_index_train = createDataPartition(y = tiss_ge$y, 
                                  times = 1, p = 0.5, list = FALSE)
tiss_ge_train_set = slice(tiss_ge, tiss_ge_index_train)
tiss_ge_test_set = slice(tiss_ge, -tiss_ge_index_train)
```

## Question 1
Below, you will find some plots and tables of the training set designed to help you develop some intuition for the data. Describe what you see. 

This is a frequency table for the tissue types in the training data. 

```{r}
table(tiss_ge_train_set$y)
```

It is difficult to make visualizations for all 100 genes in the dataset, so let's randomly sample six to focus on. 

```{r}
set.seed(49)
genes6 = sample(names(tiss_ge)[-1], 6)
genes6
```

Here are histograms of the gene expression distributions of the six genes.  

```{r}
tiss_ge_train_set %>% 
  gather(gene, expression, all_of(genes6))  %>% 
  ggplot(aes(x = expression)) + 
  geom_histogram() + 
  facet_wrap(~ gene) + 
  xlab(NULL) + ylab(NULL)
```

The boxplots below plot gene expression against tissue type for the six genes. Note that setting `scales = 'free_y'` allows the y-axis to vary from plot to plot, so they are not on the same scale. 

```{r}
tiss_ge_train_set %>% 
  gather(gene, expression, all_of(genes6))  %>% 
  ggplot(aes(x = y, y = expression)) + 
  geom_boxplot() + 
  facet_wrap(~ gene, scales = 'free_y') + 
  xlab(NULL) + ylab(NULL) + 
  scale_x_discrete(labels = str_to_title(unique(tiss_ge_train_set$y))) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

The `ggcorr` function from the `GGally` package makes pretty correlation matrix plots. Each tiny square in this plot represents the correlation between a pair of genes (out of the entire set of 100 genes). Red indicates positive correlation (close to 1), blue indicates negative correlation (close to -1), and white indicates no correlation (close to 0). 

```{r}
ggcorr(tiss_ge_train_set[,-1], hjust = 1, size = 1.5, layout.exp = 10)
```

**Solution:** 

- From the frequency table, we can see that there is a non-uniform distribution of tissue types. Some tissues, like cerebellum and kidney, are much better represented in the data than others, like endometrium and placenta. 

- The histograms show that the gene expression measurements of the six randomly sampled genes have different distributions. Some are roughly symmetric, while others are skewed. COPB1 is highly expressed in most of the tissues, while RPE65 tends to be lowly expressed. 

- The boxplots suggest that individual genes are often expressed at different levels for different tissues. However, it doesn't really seem like any one of these six genes does a great job of distinguishing between all seven of the tissue types. 

- The correlation matrix plot (lots of red and blue squares) illustrates that many of the genes in this dataset are highly correlated with each other. So, we might not need all of the predictors to do a good job of classifying the tissues. 


## Question 2

Using the `tree` function from the `tree` package and all of the training set gene expression data, build a decision tree to classify the tissue types. Get the predicted class labels for the test set data, report the test accuracy, and comment on the test confusion matrix. 

**Solution:** 

The code for fitting a classification tree follows the same syntax as fitting a regression tree in Q2.3. Because `y` is a factor variable, the `tree` function knows that it should build a classification tree. 

```{r}
fit_classtree = tree(y ~ . , data = tiss_ge_train_set)
```

To get predicted class labels and not the predicted probabilities, be sure to specify `type = "class"` in the `predict` function. 

```{r}
preds_fit_classtree = predict(fit_classtree, newdata = tiss_ge_test_set, 
                              type = "class")
```

The decision tree's test set accuracy as reported by `confusionMatrix` is 0.914, which is quite good. From the confusion matrix, we can see that our model classifies the cerebellum, colon, and hippocampus tissues in the test set perfectly, but makes a some mistakes when it comes to endometrium, kidney, liver, and placenta.  

```{r}
confusionMatrix(preds_fit_classtree, tiss_ge_test_set$y)
```

## Question 3

Fit a bagging (bootstrap aggregation) model to the training data by running `randomForest` from the `randomForest` package with the `mtry` parameter set to the number of predictors (`mtry = 100`). Be sure to set a random seed so that your code is reproducible (the solutions use `set.seed(64)`). Get the predicted class labels for the test set data and report the test accuracy. 

**Solution:** 

Setting `mtry = 100` when we have 100 predictors is what makes this model a bagging model. 

```{r}
set.seed(64)
fit_bag = randomForest(y ~ ., data = tiss_ge_train_set, mtry = 100)
```

By default, the `predict` function for `randomForest` models returns predicted class labels, but you can also explicitly specify `type = "response"`. 

```{r}
preds_fit_bag = predict(fit_bag, newdata = tiss_ge_test_set)
```

You can use the `confusionMatrix` function to calculate diagnostic metrics for the test set predictions and pull out the accuracy from the `overall` slot. 

```{r}
confusionMatrix(preds_fit_bag, tiss_ge_test_set$y)$overall[1]
```

Alternatively, you can use the predicted and true test labels to calculate the accuracy by hand. 

```{r}
mean(preds_fit_bag == tiss_ge_test_set$y)
```

Either way, the test accuracy is 0.989, which is better than the test accuracy we got from the single classification tree. 


## Question 4

Now, build a random forest model with the `mtry` parameter set to the square root of the number of predictors. Also, set `importance = TRUE` so that the importance of the predictors is assessed. You will need the variable importance information for Q3.5. Be sure to set a random seed so that your code is reproducible (the solutions use `set.seed(81)`). Get the predicted class labels for the test set data and report the test accuracy. 

**Solution:** 

For classification problems, the default value for `mtry` is the square root of the number of predictors, but we can explicitly specify `mtry = 10`.  

```{r}
set.seed(81)
fit_rf = randomForest(y ~ ., data = tiss_ge_train_set, 
                      mtry = 10, importance = TRUE)
```

As in Q3.3, you can calculate the accuracy directly or by using the `confusionMatrix` function. Here, the test accuracy is 1, meaning that all of the test set observations were classified perfectly by the model. We know from Q3.1 that the genes are highly correlated with each other. Random forests are known to decorrelate predictors, which explains the improvement over bagging. 

```{r}
preds_fit_rf = predict(fit_rf, newdata = tiss_ge_test_set)

confusionMatrix(preds_fit_rf, tiss_ge_test_set$y)$overall[1]
mean(preds_fit_rf == tiss_ge_test_set$y)
```


## Question 5

Run the `importance` function on your random forest model from Q3.4 to extract variable importance measures for each of the tissue types. Find the five most important genes for classifying kidney tissues by ordering the Gini index measures. Compare these five genes with the genes that were used to construct the classification tree in Q3.2.

Optional: Extract the five most important genes for each of the seven tissues, and compare these results with the genes that were used to construct the classification tree in Q3.2.

**Solution:** 

The five most important genes for classifying kidney tissues are COLGALT2, GPA33, CES2, PRSS3P2, and CELSR2.  

```{r}
variable_importance = importance(fit_rf)

variable_importance_kidney = 
  data.frame(gene = rownames(variable_importance), 
             Gini = variable_importance[,"kidney"])

variable_importance_kidney %>% 
  arrange(desc(Gini)) %>% head(5)
```

Six genes were used to construct the decision tree from Q3.2. Of these,  COLGALT2, GPA33, and CES2 overlap with the five most important variables for classifying kidney tissues in the random forest model. 

```{r}
summary(fit_classtree)
```

If we extract the top five genes for each of the seven tissues, we can see that all but one of the genes used to construct the classification tree are represented. The lone exception is UBOX5. There are some additional surprises; for example, CELSR2 is considered to be one of the most important genes for classifying five out of the seven tissues in random forest, but was not used by the decision tree. 

```{r}
variable_importance_long = 
  data.frame(gene = rownames(variable_importance), 
             variable_importance) %>% 
  gather(tissue, Gini, unique(tiss_ge_train_set$y))

variable_importance_long %>% 
  group_by(tissue) %>%
  slice_max(order_by = Gini, n = 5) %>% 
  ungroup() %>% 
  dplyr::select(gene) %>% 
  table() 
```
