---
title: "Modelling"
output: html_document
---

```{r, echo=FALSE, message=FALSE}
options(digits=3)
set.seed(1)
library(caret)
library(dslabs)
library(ggplot2)
library(tidyverse)
ds_theme_set()
```

## Regression vs classification

Predicting one or more classes is typically referred to as *classification*. The task of predicting a continuous variable on the other hand (for example, length of hospital stay) is typically referred to as a *regression*. 

Note that "regression models" can be used for both regression tasks and classification tasks. Don't let this throw you off!

We will begin with a linear regression, a type of model borrowed from statistics that has all of the hallmarks of machine learning (so let's call it a machine learning model!), which can be written as:

$$
\hat{y} = wX + b
$$

# Example: Predicting Sex Based on Height

Let’s consider a binary classification problem where we predict whether a person is male or female based on their height. First, let’s load the dataset and split it into training and testing sets.

```{r}
# Load the data
data(heights)
y <- heights$sex
x <- heights$height

# Split the data into training and testing sets
train_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights[train_index, ]
test_set <- heights[-train_index, ]
```

Our predictions can be denoted by $\hat{y}$ (pronounced "y hat") and our explanatory variables (or "features") denoted by $X$. In our case, we will use a single feature: height.

There are two parameters of the model that we would like to learn from the training data: $w$, weight and $b$, bias. Could we use a linear regression for our classification task? Let's try fitting a line to our outcome data.

```{r}
# Fit the linear regression model
model <- lm(as.numeric(sex == "Female") ~ height, data = train_set)

# Generate data for plotting the regression line
X_fit <- seq(min(train_set$height), max(train_set$height))
y_fit <- predict(model, newdata = data.frame(height = X_fit))

# Create a data frame for the fitted line
plot_data <- data.frame(height = X_fit, y = y_fit)

# Plot the data points and the fitted regression line
ggplot(train_set, aes(x = height, y = as.numeric(sex == "Female"))) +
  geom_point(color = 'black', shape = 4) +  # Plot data points
  geom_line(data = plot_data, aes(x = height, y = y), color = 'red', linewidth = 1) +  # Plot regression line
  labs(x = "Height", y = "Sex (Female=1, Male=0)", title = "Height vs. Sex with Linear Regression") +
  theme_minimal()
```

Linear regression places a line through a set of data points that minimizes the error between the line and the points. It is difficult to see how a meaningful threshold could be set to predict the binary outcome in our task. The predicted values can exceed our range of outcomes.

## Sigmoid function

The sigmoid function (also known as a logistic function) comes to our rescue. This function gives an "s" shaped curve that can take a number and map it into a value between 0 and 1: 

$$f : \mathbb{R} \mapsto (0,1) $$ 

The sigmoid function can be written as:

$$f(x) = \frac{1}{1+e^{-x}}$$

Let's take a look at a curve generated by this function:

```{r}
# Define the sigmoid function
sigmoid <- function(x, k = 0.1) {
  # Sigmoid function
  # Adjust k to set slope
  s <- 1 / (1 + exp(-x / k))
  return(s)
}

# Set range of values for x
x <- seq(-1, 1, length.out = 50)
y <- sigmoid(x)

# Plot the sigmoid function
data <- data.frame(x = x, y = y)
ggplot(data, aes(x = x, y = y)) +
  geom_line() +
  labs(title = "Sigmoid Function", x = "x", y = "sigmoid(x)") +
  theme_minimal()
```

We can use this to map our linear regression to produce output values that fall between 0 and 1.

$$
f(x) = \frac{1}{1+e^{-({wX + b})}}
$$

As an added benefit, we can interpret the output value as a probability. The probability relates to the positive class (the outcome with value "1"), which in our case is in-hospital mortality ("EXPIRED").

## Logistic regression

Logistic regressions are powerful models that often outperform more sophisticated machine learning models.  In machine learning studies it is typical to include performance of a logistic regression model as a baseline (as they do, for example, in [Rajkomar and colleagues](https://www.nature.com/articles/s41746-018-0029-1#Sec20)).

We need to find the parameters for the best-fitting logistic model given our data. As before, we do this with the help of a loss function that quantifies error. Our goal is to find the parameters of the model that minimise the error. With this model, we no longer use least squares due to the model's non-linear properties. Instead we will use log loss. 

## Training (or fitting) the model

As is typically the case when using machine learning packages, we don't need to code the loss function ourselves. The function is implemented as part of our machine learning package (in this case, caret). Let's try fitting a Logistic Regression to our data.

```{r}
# Fit the logistic regression model
log_model <- train(as.factor(sex == "Female") ~ height, data = train_set, method = "glm", family = "binomial")

# Generate data for plotting the regression line
X_fit <- seq(min(train_set$height), max(train_set$height), length.out = 100)
y_fit <- predict(log_model, newdata = data.frame(height = X_fit), type = "prob")
y_fit = y_fit[,2]

# Create a data frame for the fitted line
plot_data <- data.frame(height = X_fit, y = y_fit)

# Plot the data points and the fitted regression line
ggplot(train_set, aes(x = height, y = as.numeric(sex == "Female"))) +
  geom_point(color = 'black', shape = 4) +  # Plot data points
  geom_line(data = plot_data, aes(x = height, y = y), color = 'red', linewidth = 1) +  # Plot regression line
  labs(x = "Height", y = "Sex (Female=1, Male=0)", title = "Height vs. Sex with Logistic Regression") +
  theme_minimal()
```

## Decision boundary

Now that our model is able to output the probability of our outcome, we can set a decision boundary for the classification task. For example, we could classify probabilities of < 0.5 as "ALIVE" and >= 0.5 as "EXPIRED". Using this approach, we can predict outcomes for a given input.

```{r}
# Create new data for prediction
x_new <- data.frame(height = 70)

# Predict outcome
outcome <- predict(log_model, newdata = x_new)

# Predict class probabilities
probs <- predict(log_model, newdata = x_new, type = "prob")

# Print the outcome and probabilities
cat(sprintf('For height=%.1f, we predict an outcome of "%s".\n', x_new$height, outcome))
cat(sprintf('Class probabilities (Male, Female): (%.2f, %.2f).', probs[1,1], probs[1,2]))
```