---
title: "Modelling"
output: html_document
---

```{r, echo=FALSE, message=FALSE}
options(digits=3)
set.seed(1)
library(caret)
library(dslabs)
library(ggplot2)
library(tidyverse)
ds_theme_set()
```

## Regression vs classification

Predicting one or more classes is typically referred to as *classification*. The task of predicting a continuous variable on the other hand (for example, length of hospital stay) is typically referred to as a *regression*. 

Note that "regression models" can be used for both regression tasks and classification tasks. Don't let this throw you off!

We will begin with a linear regression, a type of model borrowed from statistics that has all of the hallmarks of machine learning (so let's call it a machine learning model!), which can be written as:

$$
\hat{y} = wX + b
$$

# Example: Predicting Hospital Mortality Based on Apache Score

Let’s consider a binary classification problem where we predict whether a patient will experience in-hospital mortality (actualhospitalmortality) based on their Apache Score (apachescore). First, let’s load the dataset and split it into training and testing sets.

```{r}
# Load the data
cohort <- read_csv("./eicu_cohort.csv")
y <- cohort$actualhospitalmortality
x <- cohort$apachescore

# Split the data into training and testing sets
train_index <- createDataPartition(y, times = 1, p = 0.7, list = FALSE)
train_set <- cohort[train_index, ]
test_set <- cohort[-train_index, ]
```
Our predictions can be denoted by $\hat{y}$ (pronounced "y hat") and our explanatory variables (or "features") denoted by $X$. In our case, we will use a single feature: Apache Score.

There are two parameters of the model that we would like to learn from the training data: $w$, weight and $b$, bias. Could we use a linear regression for our classification task? Let's try fitting a line to our outcome data.

```{r}
# Fit the linear regression model
model <- lm(as.numeric(actualhospitalmortality) ~ apachescore, data = train_set)

# Generate data for plotting the regression line
X_fit <- seq(min(train_set$apachescore), max(train_set$apachescore)+30, length.out = 100)
y_fit <- predict(model, newdata = data.frame(apachescore = X_fit))

# Create a data frame for the fitted line
plot_data <- data.frame(apachescore = X_fit, y = y_fit)

# Plot the data points and the fitted regression line
ggplot(train_set, aes(x = apachescore, y = actualhospitalmortality)) +
  geom_point(color = 'black', shape = 4) + 
  geom_line(data = plot_data, aes(x = apachescore, y = y), color = 'red', linewidth = 1) +
  labs(x = "Apache Score", y = "Mortality (Expired=1, Alive=0)", title = "Apache Score vs. Mortality with Linear Regression") + 
  theme_minimal()
```

Linear regression places a line through a set of data points that minimizes the error between the line and the points. It is difficult to see how a meaningful threshold could be set to predict the binary outcome in our task. The predicted values can exceed our range of outcomes.

## Sigmoid function

The sigmoid function (also known as a logistic function) comes to our rescue. This function gives an "s" shaped curve that can take a number and map it into a value between 0 and 1: 

$$f : \mathbb{R} \mapsto (0,1) $$ 

The sigmoid function can be written as:

$$f(x) = \frac{1}{1+e^{-x}}$$

Let's take a look at a curve generated by this function:

```{r}
# Define the sigmoid function
sigmoid <- function(x, k = 0.1) {
  # Sigmoid function
  # Adjust k to set slope
  s <- 1 / (1 + exp(-x / k))
  return(s)
}

# Set range of values for x
x <- seq(-1, 1, length.out = 50)
y <- sigmoid(x)

# Plot the sigmoid function
data <- data.frame(x = x, y = y)
ggplot(data, aes(x = x, y = y)) +
  geom_line() +
  labs(title = "Sigmoid Function", x = "x", y = "sigmoid(x)") +
  theme_minimal()
```

We can use this to map our linear regression to produce output values that fall between 0 and 1.

$$
f(x) = \frac{1}{1+e^{-({wX + b})}}
$$

As an added benefit, we can interpret the output value as a probability. The probability relates to the positive class (the outcome with value "1"), which in our case is in-hospital mortality.

## Exercise

A) Define regression and classification in your own words.

B) Give an example of a regression problem and a classification problem that are relevant to your area of specialism.

C) Explain why the sigmoid function is used in logistic regression.

## Solution

A) Regression is predicting a continuous outcome, while classification is predicting categorical outcomes.

B) Your examples!

C) The sigmoid function maps any real-valued number into a value between 0 and 1, making it suitable for modeling probabilities in logistic regression.

## Logistic regression

Logistic regressions are powerful models that often outperform more sophisticated machine learning models.  In machine learning studies it is typical to include performance of a logistic regression model as a baseline (as they do, for example, in [Rajkomar and colleagues](https://www.nature.com/articles/s41746-018-0029-1#Sec20)).

We need to find the parameters for the best-fitting logistic model given our data. As before, we do this with the help of a loss function that quantifies error. Our goal is to find the parameters of the model that minimise the error. With this model, we no longer use least squares due to the model's non-linear properties. Instead we will use log loss. 

## Training (or fitting) the model

As is typically the case when using machine learning packages, we don't need to code the loss function ourselves. The function is implemented as part of our machine learning package (in this case, caret). Let's try fitting a Logistic Regression to our data.

```{r}
# Fit the logistic regression model
log_model <- glm(actualhospitalmortality ~ apachescore, data = train_set, family = binomial)

# Generate data for plotting the regression line
X_fit <- seq(min(train_set$apachescore), max(train_set$apachescore)+30, length.out = 100)
y_fit <- predict(log_model, newdata = data.frame(apachescore = X_fit), type = "response")


# Create a data frame for the fitted line
plot_data <- data.frame(apachescore = X_fit, y = y_fit)

# Plot the data points and the fitted regression line
ggplot(train_set, aes(x = apachescore, y = actualhospitalmortality)) +
  geom_point(color = 'black', shape = 4) +
  geom_line(data = plot_data, aes(x = apachescore, y = y), color = 'red', linewidth = 1) +
  labs(x = "Apache Score", y = "Mortality (Expired=1, Alive=0)", title = "Apache Score vs. Mortality with Logistic Regression") +
  theme_minimal()
```

## Decision boundary

Now that our model is able to output the probability of our outcome, we can set a decision boundary for the classification task. For example, we could classify probabilities of < 0.5 as "ALIVE" and >= 0.5 as "EXPIRED". Using this approach, we can predict outcomes for a given input.

```{r}
# Create new data for prediction
x_new <- data.frame(apachescore = 120)
threshold = 0.5

# Predict outcome
probability <- predict(log_model, newdata = x_new, type = "response")

# Print the outcome and probabilities
sprintf('For apachescore=%.1f, we predict an outcome of "%s".', x_new$apachescore, ifelse(probability > threshold, "EXPIRED", "ALIVE"))
```

## Exercise

A) What is a decision boundary in the context of logistic regression?

B) How would changing the threshold (e.g., from 0.5 to 0.7) affect the predictions? Explain with an example.

## Solution

A) A decision boundary is the threshold at which the predicted probability is used to classify an outcome.

B) Changing the threshold affects the classification outcomes. For instance, increasing the threshold from 0.5 to 0.7 would result in fewer positive class predictions, making the model more conservative. 