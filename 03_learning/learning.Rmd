---
title: 'Learning'
output:
  html_document: default
  pdf_document: default
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1)
library(tidyverse)
library(dslabs)
library(caret)
ds_theme_set()
```

## How do machines learn?

How do humans learn? Typically we are given examples and we learn rules through trial and error. Machines aren't that different! In the context of machine learning, we talk about how a model "fits" to the data.

Our model has a number of tweakable parameters. We need to find the optimal values for those parameters such that our model outputs the "best" predictions for a set of input variables.

![Model training](./images/ml_model.png).

## Loss functions

Finding the best model means defining "best". We need to have some way of quantifying the difference between a "good" model (capable of making useful predictions) vs a "bad" model (not capable of making useful predictions). 

Loss functions are crucial for doing this. They allow us to quantify how closely our predictions fit to the known target values.  You will hear "objective function", "error function", and "cost function" used in a similar way. 

Mean squared error is a common example of a loss function, often used for linear regression. For each prediction, we measure the distance between the known target value ($y$) and our prediction ($y_{hat}$), and then we take the square.

```{r}
# Load necessary library
library(tidyverse)

# Create sample labeled data
data <- data.frame(x = c(1, 2, 3, 4, 5), y = c(-0.5, 1, 2, 4, 7))

# Add predictions
data$y_hat <- c(0, 2, 4, 6, 8)

# Plot the data
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  xlim(0, 6) +
  ylim(-1, 9) +
  geom_line(aes(y = y_hat), color = 'blue') +
  geom_segment(aes(x = x, xend = x, y = y, yend = y_hat), color = 'red', linetype = 'dashed') +
  annotate("text", x = 3.1, y = 3, label = "Error") +
  ggtitle("Prediction Error")
```


The further away from the data points our line gets, the bigger the error. Our best model is the one with the smallest error. Mathematically, we can define the mean squared error as:

$$
mse = \frac{1}{n}\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}
$$

$mse$ is the Mean Squared Error. $y_{i}$ is the actual value and $\hat{y}_{i}$ is the predicted value. $\sum_{}$ is notation to indicate that we are taking the sum of the difference. $n$ is the total number of observations, so $\frac{1}{n}$ indicates that we are taking the mean.

We could implement this in our code as follows:

```{r}
# Define the loss function (mean squared error)
loss <- function(y, y_hat) {
  # Calculate distances
  distances <- y - y_hat
  
  # Calculate squared distances
  squared_distances <- distances^2
  
  # Return mean squared error
  mean(squared_distances)
}

# Example usage
y <- c(-0.5, 1, 2, 4, 7)
y_hat <- c(0, 2, 4, 6, 8)
loss(y, y_hat)
```

## Minimising the error

Our goal is to find the "best" model. We have defined best as being the model with parameters that give us the smallest mean squared error. We can write this as:

$$
argmin\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}
$$

Let's stop and look at what this loss function means. We'll plot the squared error for a range of values to demonstrate how loss scales as the difference between $y$ and $\hat{y}$ increases.

```{r}
# Load necessary library
library(ggplot2)

# Generate data
x <- seq(-50, 50, 0.05)
y <- x^2

# Create the plot
ggplot(data = data.frame(x, y), aes(x = x, y = y)) +
  geom_line() +
  labs(
    x = "Difference between y and y_hat",
    y = "Loss (squared error)",
    title = "Loss Function Visualization"
  )
```

As we can see, our loss rapidly increases as predictions ($\hat{y}$) move away from the true values ($y$), because the errors are squared. The result is that outliers have a strong influence on our model fit.

## Optimisation

In machine learning, there is typically a training step where an algorithm is used to find the optimal set of model parameters (i.e. those parameters that give the minimum possible error). This is the essence of machine learning!

There are many approaches to optimisation. [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is a popular approach. 

In gradient descent we take steps in the opposite direction of the gradient of a function, seeking the lowest point (i.e. the lowest error). A learning rate allows us to define how large steps to take.

## Exercise

A) Explain in your own words what a loss function is and why it is important in machine learning.

B) Give an example of a situation where you might use Mean Squared Error (MSE) as a loss function.

C) Mean Absolute Error (MAE) is another common loss function used in machine learning. It measures the average absolute difference between predicted values and actual values. Why might MAE be a better choice than Mean Squared Error (MSE) in some cases?

## Solution

A) A loss function quantifies how well or poorly a model's predictions match the actual data. It is important because it provides a way to measure the performance of the model and guides the optimization process to improve the model's performance.

B) Mean Squared Error (MSE) is often used in regression tasks where the goal is to predict continuous values, such as predicting house prices based on various features (e.g., square footage, number of bedrooms, location).

C) Mean Absolute Error (MAE) might be a better choice than MSE when outliers are present in the data. MAE measures the average absolute difference between predicted values and actual values and is less sensitive to outliers than MSE because it does not square the error terms.

### Example of Gradient Descent

Here’s a simple example of gradient descent to minimize a quadratic function, where: 

- `f` is the function we want to minimize.
- `f_prime` is the gradient of the function.
- `gradient_descent` is the algorithm that updates x by taking steps proportional to the negative of the gradient.
- The `learning_rate` controls the size of each step, and `n_iterations` is the number of iterations to run the algorithm.
- `optimal_x` will hold the value of x that minimizes the function.

```{r}
# Define the quadratic function
f <- function(x) { x^2 + 3*x + 2 }

# Define the gradient (derivative) of the function
f_prime <- function(x) { 2*x + 3 }

# Gradient Descent algorithm
gradient_descent <- function(f_prime, initial_x, learning_rate, n_iterations) {
  x <- initial_x
  for (i in 1:n_iterations) {
    x <- x - learning_rate * f_prime(x)
  }
  return(x)
}

# Parameters
initial_x <- 0
learning_rate <- 0.1
n_iterations <- 100

# Run Gradient Descent
optimal_x <- gradient_descent(f_prime, initial_x, learning_rate, n_iterations)
optimal_x
```

## Exercise

A) What is the purpose of optimization in the context of machine learning?

B) What is gradient descent and why is it commonly used in machine learning?

C) Gradient descent is an iterative optimization algorithm used to minimize a loss function. The learning rate is a crucial hyperparameter in gradient descent that determines the size of the steps the algorithm takes towards the minimum of the loss function. Describe the impact of using a high learning rate versus a low learning rate in gradient descent. What might happen in each case, and how does it affect the convergence of the algorithm?

## Solution

A) The purpose of optimization in machine learning is to find the best model parameters that minimize the loss function, thereby improving the model's predictions and overall performance.

B) Gradient descent is an optimization algorithm used to minimize the loss function by iteratively adjusting the model parameters in the direction that reduces the loss. It is commonly used because it is computationally efficient and effective for many machine learning tasks.

C) A high learning rate in gradient descent may cause the algorithm to overshoot the optimal parameters, resulting in poor convergence or even divergence. A low learning rate, on the other hand, may lead to slow convergence, requiring more iterations to reach the optimal parameters.

## What's next?

Now that we’ve touched on how machines learn, we’ll tackle the problem of predicting the outcome of patients admitted to intensive care units in hospitals across the United States.
